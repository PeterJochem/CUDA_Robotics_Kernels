{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wYJUU-A_SfE",
        "outputId": "f9d9e2aa-af9d-474c-f0d6-1840c725f1f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting featuretools\n",
            "  Downloading featuretools-1.28.0-py3-none-any.whl (619 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.2/619.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from featuretools) (2.2.1)\n",
            "Collecting holidays<0.33,>=0.13 (from featuretools)\n",
            "  Downloading holidays-0.32-py3-none-any.whl (754 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m754.4/754.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from featuretools) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from featuretools) (23.2)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from featuretools) (1.5.3)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.10/dist-packages (from featuretools) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from featuretools) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from featuretools) (4.66.1)\n",
            "Collecting woodwork>=0.23.0 (from featuretools)\n",
            "  Downloading woodwork-0.27.0-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.1/236.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from holidays<0.33,>=0.13->featuretools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->featuretools) (2023.3.post1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from woodwork>=0.23.0->featuretools) (1.2.2)\n",
            "Requirement already satisfied: importlib-resources>=5.10.0 in /usr/local/lib/python3.10/dist-packages (from woodwork>=0.23.0->featuretools) (6.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->holidays<0.33,>=0.13->featuretools) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->woodwork>=0.23.0->featuretools) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->woodwork>=0.23.0->featuretools) (3.2.0)\n",
            "Installing collected packages: holidays, woodwork, featuretools\n",
            "  Attempting uninstall: holidays\n",
            "    Found existing installation: holidays 0.41\n",
            "    Uninstalling holidays-0.41:\n",
            "      Successfully uninstalled holidays-0.41\n",
            "Successfully installed featuretools-1.28.0 holidays-0.32 woodwork-0.27.0\n",
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.0.3-py3-none-any.whl (7.4 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.0.3\n",
            "Source files will be saved in \"/tmp/tmprd9e0e3e\".\n"
          ]
        }
      ],
      "source": [
        "!pip install -U featuretools\n",
        "!pip install nvcc4jupyter\n",
        "#!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include <stdio.h>\n",
        "#include <iostream>\n",
        "#include <stdexcept>\n",
        "#include <cassert>\n",
        "\n",
        "#define N 1000000\n",
        "#define num_threads_per_block 1024\n",
        "\n",
        "__global__ void dot_product(int* a, int* b, int* output_buffer) {\n",
        "  __shared__ int cache[num_threads_per_block];\n",
        "  int index = (blockDim.x * blockIdx.x) + threadIdx.x;\n",
        "  int cache_index = threadIdx.x;\n",
        "  //                       # blocks * # threads per block.\n",
        "  int total_num_threads = (gridDim.x * blockDim.x);\n",
        "\n",
        "  while (index < N) {\n",
        "    cache[cache_index] += a[index] * b[index];\n",
        "    index += total_num_threads;\n",
        "  }\n",
        "\n",
        "  // Wait for all the threads in the block to complete.\n",
        "  __syncthreads();\n",
        "\n",
        "  // Sum all the entries in shared memory.\n",
        "  // This can be done in O(LogN) time.\n",
        "  int left = threadIdx.x;\n",
        "  int offset = num_threads_per_block/2;\n",
        "  // The cache length must be a power of 2.\n",
        "  while (offset != 0) { // Quit when left + offset == 0\n",
        "\n",
        "    if (left < offset) {\n",
        "      cache[left] += cache[left + offset];\n",
        "    }\n",
        "    offset = offset/2;\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "\n",
        "  if (threadIdx.x == 0) {\n",
        "    // Write to the output buffer the result for this block.\n",
        "    output_buffer[blockIdx.x] = cache[0];\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "\n",
        "  int num_blocks = 14;\n",
        "  int *a_host, *b_host, *block_results_host;\n",
        "  int *a_device, *b_device, *block_results_device;\n",
        "\n",
        "  // Allocate space on the CPU for the arrays.\n",
        "  a_host = (int*) malloc(sizeof(int) * N);\n",
        "  b_host = (int*) malloc(sizeof(int) * N);\n",
        "  block_results_host = (int*) malloc(sizeof(int) * num_blocks);\n",
        "\n",
        "  // Allocate space on the GPU for the arrays.\n",
        "  cudaMalloc((void**)&a_device, sizeof(int) * N);\n",
        "  cudaMalloc((void**)&b_device, sizeof(int) * N);\n",
        "  cudaMalloc((void**)&block_results_device, sizeof(int) * num_blocks);\n",
        "\n",
        "  // Write to a and b.\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    a_host[i] = 2;\n",
        "    b_host[i] = 12;\n",
        "  }\n",
        "\n",
        "  for (int i = 0; i < num_blocks; i++) {\n",
        "    block_results_host[i] = 0;\n",
        "  }\n",
        "\n",
        "  // Copy a and b to the GPU.\n",
        "  cudaMemcpy(a_device, a_host, sizeof(int) * N, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(b_device, b_host, sizeof(int) * N, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(block_results_device, block_results_host, sizeof(int) * num_blocks, cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Run the kernel.\n",
        "  dot_product<<<1, 1>>>(a_device, b_device, block_results_device);\n",
        "\n",
        "  // Copy the results buffer back from the GPU.\n",
        "  cudaMemcpy(block_results_host, block_results_device, sizeof(int) * num_blocks, cudaMemcpyDeviceToHost);\n",
        "\n",
        "  // Sum all the entries in the result buffer.\n",
        "  int sum = 0;\n",
        "  for (int i = 0; i < num_blocks; i++) {\n",
        "    sum += block_results_host[i];\n",
        "  }\n",
        "\n",
        "  assert(sum == (2 * 12 * N));\n",
        "  std::cout << \"Works!\" << std::endl;\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-k271Lk_XzN",
        "outputId": "1725700c-f52b-4231-b1fc-6ce980c4779b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%script bash\n",
        "nvcc main.cu -o dot_product"
      ],
      "metadata": {
        "id": "XU2H38Ax_YFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script bash\n",
        "./dot_product"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M5Zzom__gyd",
        "outputId": "7874de43-bfa0-49d8-ec50-f286073b4c18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Works!\n"
          ]
        }
      ]
    }
  ]
}